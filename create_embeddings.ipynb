{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Der Code unterhalb braucht nur gensim, kann mit `pip install gensim` installiert werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from gensim.models import Word2Vec\n",
    "import subprocess\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec trainieren\n",
    "\n",
    "Der Code unterhalb ist nur relevant, wenn du das Modell neu trainieren willst - ein fertiges Modell ist allerdings schon geuploaded. \n",
    "\n",
    "Der Code zum Testen / Anwenden ist weiter unten. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when reading file XML/._touchepasamastatue_output.xml\n",
      "Error when reading file XML/._novelumcarcassonne_output.xml\n",
      "Error when reading file XML/._normaux_output.xml\n",
      "Error when reading file XML/._Nemesis2_output.xml\n",
      "Error when reading file XML/._Hélix_output.xml\n",
      "Error when reading file XML/._Mora_output.xml\n",
      "Error when reading file XML/._Destoursetdeslys_output.xml\n",
      "Error when reading file XML/._meduanocta_output.xml\n",
      "Error when reading file XML/._Nemesis_output.xml\n",
      "Error when reading file XML/._braves_output.xml\n",
      "Error when reading file XML/._Alvarium_output.xml\n",
      "Error when reading file XML/._natifs_output.xml\n",
      "Error when reading file XML/._patriaalbiges_output.xml\n",
      "Error when reading file XML/._ClermontNC_output.xml\n",
      "Error when reading file XML/._Furie_output.xml\n",
      "Error when reading file XML/._tenesoun_output.xml\n",
      "Error when reading file XML/._GUD_output.xml\n",
      "Error when reading file XML/._remparts_output.xml\n",
      "Error when reading file XML/._Korser_output.xml\n",
      "Error when reading file XML/._maquis_output.xml\n",
      "Error when reading file XML/Zentropa/._Zentropa11_output.xml\n",
      "Error when reading file XML/Zentropa/._Zentropa6_output.xml\n",
      "Error when reading file XML/Zentropa/._Zentropa3_output.xml\n",
      "Error when reading file XML/Zentropa/._Zentropa2_output.xml\n",
      "Error when reading file XML/Zentropa/._Zentropa7_output.xml\n",
      "Error when reading file XML/Zentropa/._Zentropa5_output.xml\n",
      "Error when reading file XML/Zentropa/._Zentropa12_output.xml\n",
      "Error when reading file XML/Zentropa/._Zentropa_output.xml\n",
      "Error when reading file XML/Zentropa/._Zentropa4_output.xml\n",
      "Error when reading file XML/Zentropa/._Zentropa10_output.xml\n",
      "Error when reading file XML/Zentropa/._Zentropa9_output.xml\n",
      "Error when reading file XML/Zentropa/._Zentropa8_output.xml\n",
      "Error when reading file XML/Iliade/._Iliade4_output.xml\n",
      "Error when reading file XML/Iliade/._Iliade3_output.xml\n",
      "Error when reading file XML/Iliade/._Iliade5_output.xml\n",
      "Error when reading file XML/Iliade/._Iliade_output.xml\n",
      "Error when reading file XML/Iliade/._Iliade8_output.xml\n",
      "Error when reading file XML/Iliade/._Iliade9_output.xml\n",
      "Error when reading file XML/Iliade/._Iliade2_output.xml\n",
      "Error when reading file XML/Iliade/._Iliade6_output.xml\n",
      "Error when reading file XML/Iliade/._Iliade7_output.xml\n",
      "Error when reading file XML/La cocarde etudiante/._cocardeetudiante3_output.xml\n",
      "Error when reading file XML/La cocarde etudiante/._Cocardeetudiante_output.xml\n",
      "Error when reading file XML/La cocarde etudiante/._cocardeetudiante2_output.xml\n",
      "Error when reading file XML/Academia Christiana/._AcademiaC2_output.xml\n",
      "Error when reading file XML/Academia Christiana/._AcademiaC_output.xml\n",
      "Error when reading file XML/Temeraires/._temeraires_output.xml\n",
      "Error when reading file XML/Temeraires/._temeraires2_output.xml\n"
     ]
    }
   ],
   "source": [
    "def cleanup(text:str, re_cleanup:bool, lowercase:bool)->str: \n",
    "    if lowercase: \n",
    "            text = text.lower()\n",
    "    if re_cleanup: \n",
    "        # Replace all punctuation chars with a single whitespace, keep only normal words and digits\n",
    "        text = re.sub(r\"[^\\w\\n\\d']+\", ' ', text)\n",
    "        text = re.sub(r\"^\\s+|\\s+(?=\\n)\", '', text, flags=re.MULTILINE) # Delete leading/trailing whitespaces\n",
    "    text = re.sub(r'[^(\\n|\\S)]+', ' ', text) # Clean up: spaces (except newlines) to single space\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_text_from_xml(file_path):\n",
    "    try:\n",
    "        # Parse the XML file\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # Ensure the XML has the expected format (root is <body>)\n",
    "        if root.tag != 'body':\n",
    "            return []\n",
    "\n",
    "        # Extract text content from all <text> elements\n",
    "        text_elements = root.findall('.//text')\n",
    "        texts = [cleanup(elem.text, re_cleanup=True, lowercase=True) for elem in text_elements if elem.text]\n",
    "        return texts\n",
    "    except Exception as e:\n",
    "        print(f\"Error when reading file {file_path}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    all_texts = []\n",
    "\n",
    "    # Walk through all files and subdirectories\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            # Check if the file is an .xml file\n",
    "            if file.endswith('.xml'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                # Extract text from the XML file\n",
    "                extracted_texts = extract_text_from_xml(file_path)\n",
    "                all_texts.extend(extracted_texts)\n",
    "\n",
    "    return all_texts\n",
    "\n",
    "# Specify the folder to process\n",
    "folder_path = 'XML'\n",
    "\n",
    "# Process the folder and print the results\n",
    "all_extracted_texts = process_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_corpus.txt\", 'w') as fw: \n",
    "    fw.writelines('\\n'.join(all_extracted_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=\"cat training_corpus.txt | tr '[:upper:]' '[:lower:]' | tr -s '[:space:]' '\\\\n' | tr -d '[:punct:]' | grep -v '[0-9]' | sort | uniq -c | awk '$1 > 2 {print $2}' > words_preprocessed.txt\", returncode=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write all word to a file which appear at least k times\n",
    "k = 2\n",
    "\n",
    "command = f\"cat training_corpus.txt | tr '[:upper:]' '[:lower:]' | tr -s '[:space:]' '\\\\n' | tr -d '[:punct:]' | grep -v '[0-9]' | sort | uniq -c | awk '$1 > {k} {{print $2}}' > words_preprocessed.txt\"\n",
    "subprocess.run(command, shell=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "model = Word2Vec(\n",
    "    corpus_file=\"training_corpus.txt\",\n",
    "    vector_size=100,  # Dimensionality of the word embeddings\n",
    "    window=5,         # Max distance between current and predicted word\n",
    "    min_count=3,      # Ignores all words with total frequency < 3\n",
    "    workers=4,        # Number of threads to run in parallel\n",
    "    epochs=5,          # Number of training epochs\n",
    "    sg=1\n",
    ")\n",
    "\n",
    "# 4) Save the trained model for later use\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modell anwenden\n",
    "\n",
    "Eigentlich können nur Embeddings für einzelne Wörter erstellt werden - es können allerdings mehrere einzelne Embeddings aufaddiertt werden, auch wenn das Ergebnis dann eher so mittel ist. \n",
    "\n",
    "Generell wirkt das Ergebnis recht mittelmäßig, vmtl. weil nicht genug Daten vorhanden sind. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('wars', 0.9887760877609253), ('novel', 0.9866569638252258), ('wolf', 0.9847815632820129), ('originally', 0.9843636155128479), ('frost', 0.9841103553771973), ('civilians', 0.9840000867843628), ('illustrator', 0.9839940667152405), ('earlier', 0.9831578135490417), ('hunting', 0.9830578565597534), ('illustrated', 0.9830324649810791)]\n"
     ]
    }
   ],
   "source": [
    "# Combine two vectors\n",
    "\n",
    "# Get word vectors\n",
    "v1 = model.wv['star']  \n",
    "v2 = model.wv['wars'] \n",
    "# Combine word vectors\n",
    "v_combined = (v1 + v2) / 2\n",
    "\n",
    "\n",
    "similar_words = model.wv.most_similar(v_combined, topn=10)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('invasion', 0.6447374820709229), ('ouest', 0.6436580419540405), ('occident', 0.6303271651268005), ('enracinement', 0.6251101493835449), ('turquie', 0.6235107779502869), ('ours', 0.6135778427124023), ('frontières', 0.61162930727005), ('islam', 0.6114954352378845), ('asie', 0.6085038781166077), ('mémorables', 0.6056879162788391)]\n"
     ]
    }
   ],
   "source": [
    "# Find nearest neighbours for single word\n",
    "\n",
    "similar_words = model.wv.most_similar(\"europe\", topn=10)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check similarity between two words\n",
    "similarity = model.wv.similarity(\"word1\", \"word2\")\n",
    "print(similarity)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
