{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Der Code unterhalb braucht nur gensim, kann mit `pip install gensim` installiert werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec trainieren\n",
    "\n",
    "Der Code unterhalb ist nur relevant, wenn du das Modell neu trainieren willst - ein fertiges Modell ist allerdings schon geuploaded. \n",
    "\n",
    "Der Code zum Testen / Anwenden ist weiter unten. \n",
    "\n",
    "Ich habe die Einstellungen nochmal ein bisschen verändert: \n",
    "100 ist vollkommen ausreichend als dimension, und ein größeres window erreicht, dass mehr Information rausgeholt wird.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "model = FastText(\n",
    "    corpus_file=\"corpus_cleaned.txt\",\n",
    "    vector_size=300,  # Dimensionality of the word embeddings\n",
    "    window=10,         # Max distance between current and predicted word\n",
    "    min_count=3,      # Ignores all words with total frequency < 3\n",
    "    workers=4,        # Number of threads to run in parallel\n",
    "    epochs=7,        # Number of training epochs\n",
    "    sg=1\n",
    ")\n",
    "\n",
    "# 4) Save the trained model for later use\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume `model` is your trained Word2Vec model\n",
    "vocabulary = model.wv.key_to_index\n",
    "\n",
    "# Print the vocabulary (all words)\n",
    "with open(\"vocabulary.txt\", 'w') as fw: \n",
    "    fw.writelines('\\n'.join(list(vocabulary.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modell anwenden\n",
    "\n",
    "Wenn in fastText-Modell verwendet wird (ist beim aktuellen word2vec-Modell und dem zugehörigen Code oberhalb der Fall), können beliebige Sequenzen und Wörter getestet werden, unabhängig davon, ob diese in den Trainingsdaten waren. In diesem Fall wird die Eingabe in einzelne, kleinere Buchstabenfolgen zerlegt, die sich in den Trainingsdaten finden. Trotzdem ist das Ergebnis auf Wörtern aus dem Vokabular (kann mit dem Code direkt oberhalb in eine Datei geschrieben werden) vermutlich ab besten. \n",
    "\n",
    "Wenn ein normales Word2Vec-Modell verwendet wird (nicht empfohlen, schlechtere Ergebnisse), können nur Wörter aus dem Vokabular eingegeben werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_contexts(\n",
    "    file_path: str | Path,\n",
    "    target_word: str,\n",
    "    window: int = 3,\n",
    "    max_examples: int = 5\n",
    ") -> None:\n",
    "    \"\"\"Zählt Vorkommen von `target_word` und gibt Kontexte (±window) aus.\"\"\"\n",
    "    text = Path(file_path).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    sentences = text.split('\\n')\n",
    "    matches = []\n",
    "    target_word = target_word.lower()\n",
    "    for s_idx, sent in enumerate(sentences): \n",
    "        sent_lower = sent.lower()\n",
    "        if target_word in sent_lower: # matches\n",
    "            for i, char in enumerate(sent_lower): \n",
    "                if sent_lower[i:i + len(target_word)] == target_word: \n",
    "                    matches.append((i, sent))\n",
    "\n",
    "\n",
    "    sampled_matches = random.sample(matches, k=min(5, len(matches)))\n",
    "    print(f\"Found {len(matches)} matches for '{target_word}' in {file_path}.\")\n",
    "    for n, match_el in enumerate(sampled_matches):\n",
    "        idx = match_el[0]\n",
    "        sent = match_el[1]\n",
    "        start = max(idx - window, 0)\n",
    "        end = min(idx + window + len(target_word) +1, len(sent))\n",
    "        \n",
    "        out_text = sent[idx:idx+len(target_word)].upper()\n",
    "        if start < idx: \n",
    "            out_text = sent[start:idx] + out_text\n",
    "        if idx < end: \n",
    "            out_text = out_text + sent[idx+len(target_word):end]\n",
    "        if start > 0: \n",
    "            out_text = '... ' + out_text\n",
    "        if end < len(sent): \n",
    "            out_text = out_text + ' ...'\n",
    "        print(out_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model = FastText.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('paupérisation', 0.925370991230011), ('valorisation', 0.9064534902572632), ('atomisation', 0.9040836095809937), ('dématérialisation', 0.8991129994392395), ('maximisation', 0.8972547054290771), ('légalisation', 0.8854812383651733), ('robotisation', 0.8785582780838013), ('stigmatisation', 0.8775557279586792), ('pasteurisation', 0.8766663670539856), ('utilisation', 0.8766486644744873), ('désacralisation', 0.8733818531036377), ('automatisation', 0.8690922856330872), ('crétinisation', 0.8671633005142212), ('banalisation', 0.867033839225769), ('déshumanisation', 0.8663885593414307), ('virtualisation', 0.8648918867111206), ('globalisation', 0.8630098700523376), ('autorisation', 0.862382709980011), ('uniformisation', 0.8612636923789978), ('privatisation', 0.8587686419487), ('relocalisation', 0.8577152490615845), ('centralisation', 0.8570511937141418), ('désindustrialisation', 0.8566339015960693), ('médicalisation', 0.8553164005279541), ('marchandisation', 0.853294312953949), ('urbanisation', 0.8511891961097717), ('standardisation', 0.8492730259895325), ('cristallisation', 0.8484847545623779), ('réinitialisation', 0.8480150699615479), ('synchronisation', 0.8427519202232361), ('anglicisation', 0.8419333696365356), ('industrialisation', 0.8401752710342407), ('artificialisation', 0.8377707600593567), ('déterritorialisation', 0.8324288725852966), ('américanisation', 0.8320960998535156), ('culpabilisation', 0.8305312395095825), ('victimisation', 0.8290782570838928), ('financiarisation', 0.8269574046134949), ('spéculation', 0.8231069445610046), ('réalisation', 0.8230091333389282), ('modernisation', 0.8196973204612732), ('sensation', 0.818656861782074), ('instrumentalisation', 0.817777156829834), ('généralisation', 0.81771320104599), ('ségrégation', 0.8156137466430664), ('délation', 0.8150843381881714), ('immatriculation', 0.810723066329956), ('élévation', 0.8090409636497498), ('régulation', 0.8083345293998718), ('délégation', 0.8044252395629883), ('africanisation', 0.8029231429100037), ('négation', 0.8013741970062256), ('mondialisation', 0.797768235206604), ('accumulation', 0.7967326641082764), ('évaluation', 0.7965207695960999), ('circulation', 0.7963437438011169), ('détérioration', 0.7959339618682861), ('aimable_autorisation', 0.794723391532898), ('canonisation', 0.7930285930633545), ('éjaculation', 0.7913965582847595)] \n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'find_contexts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m similar_words \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mmost_similar(target_word, topn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(similar_words, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mfind_contexts\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorpus_raw.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, target_word, window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m70\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# find_contexts(\"corpus_cleaned.txt\", target_word, window=70) # Findet möglicherweise manche Wörter, die in `corpus_raw.txt` durch Punktiation o.ä. getrennt sind. Außerdem sind die gemergeden Phrasen dort bereits mit einem Unterstrich kombiniert, anders als in `corpus_raw.txt`.\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'find_contexts' is not defined"
     ]
    }
   ],
   "source": [
    "target_word = 'numérisation'\n",
    "similar_words = model.wv.most_similar(target_word, topn=60)\n",
    "print(similar_words, '\\n')\n",
    "find_contexts(\"corpus_raw.txt\", target_word, window=70)\n",
    "# find_contexts(\"corpus_cleaned.txt\", target_word, window=70) # Findet möglicherweise manche Wörter, die in `corpus_raw.txt` durch Punktiation o.ä. getrennt sind. Außerdem sind die gemergeden Phrasen dort bereits mit einem Unterstrich kombiniert, anders als in `corpus_raw.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_overlap(target_word, neighbours:list): \n",
    "    overlaps = []\n",
    "    lens = []\n",
    "    for neighbour in neighbours: \n",
    "        if len(target_word) > len(neighbour): \n",
    "            longer_word = target_word\n",
    "            shorter_word = neighbour\n",
    "        else: \n",
    "            longer_word = neighbour\n",
    "            shorter_word = target_word\n",
    "        lens.append(len(shorter_word))\n",
    "        n_gram_len = len(shorter_word)\n",
    "        found_overlap = False\n",
    "        print(neighbour)\n",
    "        while n_gram_len > 0 and not found_overlap: \n",
    "            for starting_pos in range(len(shorter_word) - n_gram_len + 1): \n",
    "                #print(starting_pos)\n",
    "                n_gram = shorter_word[starting_pos:starting_pos+n_gram_len]\n",
    "                #print(n_gram)\n",
    "                if n_gram in longer_word: \n",
    "                    #print('yes')\n",
    "                    overlaps.append(n_gram_len)\n",
    "                    found_overlap = True\n",
    "                    break\n",
    "            n_gram_len -= 1\n",
    "        if n_gram_len == 0 and not found_overlap: \n",
    "            overlaps.append(0)\n",
    "    \n",
    "    print(overlaps)\n",
    "    print(lens)\n",
    "    sum = 0\n",
    "    for i, overlap in enumerate(overlaps): \n",
    "        # Prints fraction of *maximum possible overlap* for those words \n",
    "        sum += overlap/lens[i]\n",
    "    print(sum/len(overlaps))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
