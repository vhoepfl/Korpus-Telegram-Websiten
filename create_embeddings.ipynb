{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Der Code unterhalb braucht nur gensim, kann mit `pip install gensim` installiert werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "import subprocess\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec trainieren\n",
    "\n",
    "Der Code unterhalb ist nur relevant, wenn du das Modell neu trainieren willst - ein fertiges Modell ist allerdings schon geuploaded. \n",
    "\n",
    "Der Code zum Testen / Anwenden ist weiter unten. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read file XML/touchepasamastatue_output.xml\n",
      "Error when reading file XML/._touchepasamastatue_output.xml\n",
      "Successfully read file XML/Hélix_output.xml\n",
      "Error when reading file XML/._novelumcarcassonne_output.xml\n",
      "Successfully read file XML/maquis_output.xml\n",
      "Error when reading file XML/._normaux_output.xml\n",
      "Successfully read file XML/meduanocta_output.xml\n",
      "Error when reading file XML/._Nemesis2_output.xml\n",
      "Successfully read file XML/braves_output.xml\n",
      "Error when reading file XML/._Hélix_output.xml\n",
      "Error when reading file XML/._Mora_output.xml\n",
      "Error when reading file XML/._Destoursetdeslys_output.xml\n",
      "Successfully read file XML/Destoursetdeslys_output.xml\n",
      "Error when reading file XML/._meduanocta_output.xml\n",
      "Successfully read file XML/Alvarium_output.xml\n",
      "Successfully read file XML/Nemesis2_output.xml\n",
      "Successfully read file XML/normaux_output.xml\n",
      "Successfully read file XML/tenesoun_output.xml\n",
      "Error when reading file XML/._Nemesis_output.xml\n",
      "Successfully read file XML/novelumcarcassonne_output.xml\n",
      "Error when reading file XML/._braves_output.xml\n",
      "Successfully read file XML/GUD_output.xml\n",
      "Error when reading file XML/._Alvarium_output.xml\n",
      "Successfully read file XML/Mora_output.xml\n",
      "Successfully read file XML/Furie_output.xml\n",
      "Error when reading file XML/._natifs_output.xml\n",
      "Successfully read file XML/natifs_output.xml\n",
      "Error when reading file XML/._patriaalbiges_output.xml\n",
      "Successfully read file XML/patriaalbiges_output.xml\n",
      "Error when reading file XML/._ClermontNC_output.xml\n",
      "Error when reading file XML/._Furie_output.xml\n",
      "Error when reading file XML/._tenesoun_output.xml\n",
      "Error when reading file XML/._GUD_output.xml\n",
      "Error when reading file XML/._remparts_output.xml\n",
      "Successfully read file XML/remparts_output.xml\n",
      "Error when reading file XML/._Korser_output.xml\n",
      "Successfully read file XML/Nemesis_output.xml\n",
      "Error when reading file XML/._maquis_output.xml\n",
      "Successfully read file XML/ClermontNC_output.xml\n",
      "Successfully read file XML/Korser_output.xml\n",
      "Error when reading file XML/Zentropa/._Zentropa11_output.xml\n",
      "Successfully read file XML/Zentropa/Zentropa10_output.xml\n",
      "Error when reading file XML/Zentropa/._Zentropa6_output.xml\n",
      "Error when reading file XML/Zentropa/._Zentropa3_output.xml\n",
      "Successfully read file XML/Zentropa/Zentropa6_output.xml\n",
      "Error when reading file XML/Zentropa/._Zentropa2_output.xml\n",
      "Successfully read file XML/Zentropa/Zentropa12_output.xml\n",
      "Error when reading file XML/Zentropa/._Zentropa7_output.xml\n",
      "Successfully read file XML/Zentropa/Zentropa_output.xml\n",
      "Successfully read file XML/Zentropa/Zentropa11_output.xml\n",
      "Error when reading file XML/Zentropa/._Zentropa5_output.xml\n",
      "Error when reading file XML/Zentropa/._Zentropa12_output.xml\n",
      "Successfully read file XML/Zentropa/Zentropa7_output.xml\n",
      "Error when reading file XML/Zentropa/._Zentropa_output.xml\n",
      "Successfully read file XML/Zentropa/Zentropa4_output.xml\n",
      "Error when reading file XML/Zentropa/._Zentropa4_output.xml\n",
      "Successfully read file XML/Zentropa/Zentropa5_output.xml\n",
      "Successfully read file XML/Zentropa/Zentropa8_output.xml\n",
      "Successfully read file XML/Zentropa/Zentropa3_output.xml\n",
      "Error when reading file XML/Zentropa/._Zentropa10_output.xml\n",
      "Error when reading file XML/Zentropa/._Zentropa9_output.xml\n",
      "Error when reading file XML/Zentropa/._Zentropa8_output.xml\n",
      "Successfully read file XML/Zentropa/Zentropa2_output.xml\n",
      "Successfully read file XML/Zentropa/Zentropa9_output.xml\n",
      "Successfully read file XML/Iliade/Iliade_output.xml\n",
      "Successfully read file XML/Iliade/Iliade5_output.xml\n",
      "Successfully read file XML/Iliade/Iliade9_output.xml\n",
      "Error when reading file XML/Iliade/._Iliade4_output.xml\n",
      "Error when reading file XML/Iliade/._Iliade3_output.xml\n",
      "Error when reading file XML/Iliade/._Iliade5_output.xml\n",
      "Error when reading file XML/Iliade/._Iliade_output.xml\n",
      "Error when reading file XML/Iliade/._Iliade8_output.xml\n",
      "Successfully read file XML/Iliade/Iliade8_output.xml\n",
      "Error when reading file XML/Iliade/._Iliade9_output.xml\n",
      "Successfully read file XML/Iliade/Iliade4_output.xml\n",
      "Successfully read file XML/Iliade/Iliade6_output.xml\n",
      "Error when reading file XML/Iliade/._Iliade2_output.xml\n",
      "Error when reading file XML/Iliade/._Iliade6_output.xml\n",
      "Successfully read file XML/Iliade/Iliade3_output.xml\n",
      "Successfully read file XML/Iliade/Iliade2_output.xml\n",
      "Error when reading file XML/Iliade/._Iliade7_output.xml\n",
      "Successfully read file XML/Iliade/Iliade7_output.xml\n",
      "Error when reading file XML/La cocarde etudiante/._cocardeetudiante3_output.xml\n",
      "Error when reading file XML/La cocarde etudiante/._Cocardeetudiante_output.xml\n",
      "Error when reading file XML/La cocarde etudiante/._cocardeetudiante2_output.xml\n",
      "Successfully read file XML/La cocarde etudiante/cocardeetudiante2_output.xml\n",
      "Successfully read file XML/La cocarde etudiante/Cocardeetudiante_output.xml\n",
      "Successfully read file XML/La cocarde etudiante/cocardeetudiante3_output.xml\n",
      "Successfully read file XML/Academia Christiana/AcademiaC2_output.xml\n",
      "Error when reading file XML/Academia Christiana/._AcademiaC2_output.xml\n",
      "Successfully read file XML/Academia Christiana/AcademiaC_output.xml\n",
      "Error when reading file XML/Academia Christiana/._AcademiaC_output.xml\n",
      "Successfully read file XML/Temeraires/temeraires2_output.xml\n",
      "Successfully read file XML/Temeraires/temeraires_output.xml\n",
      "Error when reading file XML/Temeraires/._temeraires_output.xml\n",
      "Error when reading file XML/Temeraires/._temeraires2_output.xml\n"
     ]
    }
   ],
   "source": [
    "def cleanup(text:str, re_cleanup:bool, lowercase:bool)->str: \n",
    "    if lowercase: \n",
    "            text = text.lower()\n",
    "    if re_cleanup: \n",
    "        # Replace all punctuation chars with a single whitespace, keep only normal words and digits\n",
    "        text = re.sub(r\"[^\\w\\n\\d]+\", ' ', text)\n",
    "        text = re.sub(r\"^\\s+|\\s+(?=\\n)\", '', text, flags=re.MULTILINE) # Delete leading/trailing whitespaces\n",
    "    text = re.sub(r'[^(\\n|\\S)]+', ' ', text) # Clean up: spaces (except newlines) to single space\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_text_from_xml(file_path):\n",
    "    try:\n",
    "        # Parse the XML file\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # Ensure the XML has the expected format (root is <body>)\n",
    "        if root.tag != 'body':\n",
    "            return []\n",
    "\n",
    "        # Extract text content from all <text> elements\n",
    "        text_elements = root.findall('.//text')\n",
    "        texts = [cleanup(elem.text, re_cleanup=True, lowercase=True) for elem in text_elements if elem.text]\n",
    "        print(f\"Successfully read file {file_path}\")\n",
    "        return texts\n",
    "    except Exception as e:\n",
    "        print(f\"Error when reading file {file_path}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    all_texts = []\n",
    "\n",
    "    # Walk through all files and subdirectories\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            # Check if the file is an .xml file\n",
    "            if file.endswith('.xml'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                # Extract text from the XML file\n",
    "                extracted_texts = extract_text_from_xml(file_path)\n",
    "                all_texts.extend(extracted_texts)\n",
    "\n",
    "    return all_texts\n",
    "\n",
    "# Specify the folder to process\n",
    "folder_path = 'XML'\n",
    "\n",
    "# Process the folder and print the results\n",
    "all_extracted_texts = process_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_corpus.txt\", 'w') as fw: \n",
    "    fw.writelines('\\n'.join(all_extracted_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train Model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mFastText\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_corpus.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvector_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Dimensionality of the word embeddings\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# Max distance between current and predicted word\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Ignores all words with total frequency < 3\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Number of threads to run in parallel\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Number of training epochs\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 4) Save the trained model for later use\u001b[39;00m\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword2vec.model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/hiwijob/Laura/word2vec/.venv/lib/python3.11/site-packages/gensim/models/fasttext.py:435\u001b[0m, in \u001b[0;36mFastText.__init__\u001b[0;34m(self, sentences, corpus_file, sg, hs, vector_size, alpha, window, min_count, max_vocab_size, word_ngrams, sample, seed, workers, min_alpha, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, min_n, max_n, sorted_vocab, bucket, trim_rule, batch_words, callbacks, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mvectors_vocab_lockf \u001b[38;5;241m=\u001b[39m ones(\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mREAL)\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mvectors_ngrams_lockf \u001b[38;5;241m=\u001b[39m ones(\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mREAL)\n\u001b[0;32m--> 435\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mFastText\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43msentences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvector_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrim_rule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrim_rule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_vocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_vocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_final_vocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_final_vocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msorted_vocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msorted_vocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnull_word\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnull_word\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mns_exponent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mns_exponent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhashfxn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhashfxn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnegative\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbow_mean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcbow_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_alpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshrink_windows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshrink_windows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/hiwijob/Laura/word2vec/.venv/lib/python3.11/site-packages/gensim/models/word2vec.py:430\u001b[0m, in \u001b[0;36mWord2Vec.__init__\u001b[0;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, passes\u001b[38;5;241m=\u001b[39m(epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_vocab(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, trim_rule\u001b[38;5;241m=\u001b[39mtrim_rule)\n\u001b[0;32m--> 430\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus_total_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mend_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_alpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trim_rule \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/hiwijob/Laura/word2vec/.venv/lib/python3.11/site-packages/gensim/models/word2vec.py:1078\u001b[0m, in \u001b[0;36mWord2Vec.train\u001b[0;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1073\u001b[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_epoch(\n\u001b[1;32m   1074\u001b[0m         corpus_iterable, cur_epoch\u001b[38;5;241m=\u001b[39mcur_epoch, total_examples\u001b[38;5;241m=\u001b[39mtotal_examples,\n\u001b[1;32m   1075\u001b[0m         total_words\u001b[38;5;241m=\u001b[39mtotal_words, queue_factor\u001b[38;5;241m=\u001b[39mqueue_factor, report_delay\u001b[38;5;241m=\u001b[39mreport_delay,\n\u001b[1;32m   1076\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1077\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1078\u001b[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch_corpusfile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m trained_word_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m trained_word_count_epoch\n\u001b[1;32m   1083\u001b[0m raw_word_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m raw_word_count_epoch\n",
      "File \u001b[0;32m~/hiwijob/Laura/word2vec/.venv/lib/python3.11/site-packages/gensim/models/word2vec.py:1376\u001b[0m, in \u001b[0;36mWord2Vec._train_epoch_corpusfile\u001b[0;34m(self, corpus_file, cur_epoch, total_examples, total_words, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1373\u001b[0m     thread\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1374\u001b[0m     thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m-> 1376\u001b[0m trained_word_count, raw_word_count, job_tally \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_epoch_progress\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_queue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_queue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_queue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_corpus_file_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trained_word_count, raw_word_count, job_tally\n",
      "File \u001b[0;32m~/hiwijob/Laura/word2vec/.venv/lib/python3.11/site-packages/gensim/models/word2vec.py:1289\u001b[0m, in \u001b[0;36mWord2Vec._log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[1;32m   1286\u001b[0m unfinished_worker_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m unfinished_worker_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1289\u001b[0m     report \u001b[38;5;241m=\u001b[39m \u001b[43mprogress_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# blocks if workers too slow\u001b[39;00m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m report \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# a thread reporting that it finished\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m         unfinished_worker_count \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "model = FastText(\n",
    "    corpus_file=\"training_corpus.txt\",\n",
    "    vector_size=300,  # Dimensionality of the word embeddings\n",
    "    window=5,         # Max distance between current and predicted word\n",
    "    min_count=3,      # Ignores all words with total frequency < 3\n",
    "    workers=4,        # Number of threads to run in parallel\n",
    "    epochs=5,        # Number of training epochs\n",
    "    sg=1\n",
    ")\n",
    "\n",
    "# 4) Save the trained model for later use\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume `model` is your trained Word2Vec model\n",
    "vocabulary = model.wv.key_to_index\n",
    "\n",
    "# Print the vocabulary (all words)\n",
    "with open(\"vocabulary.txt\", 'w') as fw: \n",
    "    fw.writelines('\\n'.join(list(vocabulary.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modell anwenden\n",
    "\n",
    "Wenn in fastText-Modell verwendet wird (ist beim aktuellen word2vec-Modell und dem zugehörigen Code oberhalb der Fall), können beliebige Sequenzen und Wörter getestet werden, unabhängig davon, ob diese in den Trainingsdaten waren. In diesem Fall wird die Eingabe in einzelne, kleinere Buchstabenfolgen zerlegt, die sich in den Trainingsdaten finden. Trotzdem ist das Ergebnis auf Wörtern aus dem Vokabular (kann mit dem Code direkt oberhalb in eine Datei geschrieben werden) vermutlich ab besten. \n",
    "\n",
    "Wenn ein normales Word2Vec-Modell verwendet wird (nicht empfohlen, schlechtere Ergebnisse), können nur Wörter aus dem Vokabular eingegeben werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model = FastText.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('alarme', 0.8317347764968872), ('vacarme', 0.7709546685218811), ('armel', 0.7630341649055481), ('larme', 0.7539097666740417), ('parme', 0.7298886179924011), ('carmel', 0.6511124968528748), ('armenie', 0.6490821838378906), ('ariège', 0.6351330876350403), ('handicapée', 0.6339969635009766), ('envergure', 0.6334922909736633), ('bolide', 0.6313551664352417), ('algorithme', 0.6310719847679138), ('enorme', 0.6272657513618469), ('aveugle', 0.6268576979637146), ('angoisse', 0.6240061521530151), ('pénitentiaire', 0.6237596869468689), ('enferme', 0.6236466765403748), ('suicidaire', 0.623491644859314), ('arène', 0.6218565106391907), ('allègre', 0.6210903525352478)]\n"
     ]
    }
   ],
   "source": [
    "# Find nearest neighbours for single word\n",
    "\n",
    "similar_words = model.wv.most_similar(\"arme\", topn=20)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.030709\n"
     ]
    }
   ],
   "source": [
    "# Check similarity between two words\n",
    "similarity = model.wv.similarity(\"aujourd'hui\", \"a\")\n",
    "print(similarity)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
